{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825277f1-ad90-4e55-8e15-3b9af6102abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"wikipedia\", \"20220301.en\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b220f6d-97cc-4025-a2f5-dfe8f8412e0a",
   "metadata": {},
   "source": [
    "### Loading the wikipedia English text Data available from Hugging Face\n",
    "\n",
    "https://huggingface.co/datasets/wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1cd4658-9eb7-4f11-a416-38a4ed8237c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf130e7-227f-413f-a37c-4c4d06232942",
   "metadata": {},
   "source": [
    "Converting DatasetDict type to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e862c1-d8fe-40ee-80bb-90edd9e12320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                      url      title  \\\n",
      "0   12  https://en.wikipedia.org/wiki/Anarchism  Anarchism   \n",
      "1   25     https://en.wikipedia.org/wiki/Autism     Autism   \n",
      "2   39     https://en.wikipedia.org/wiki/Albedo     Albedo   \n",
      "3  290          https://en.wikipedia.org/wiki/A          A   \n",
      "4  303    https://en.wikipedia.org/wiki/Alabama    Alabama   \n",
      "\n",
      "                                                text  \n",
      "0  Anarchism is a political philosophy and moveme...  \n",
      "1  Autism is a neurodevelopmental disorder charac...  \n",
      "2  Albedo (; ) is the measure of the diffuse refl...  \n",
      "3  A, or a, is the first letter and the first vow...  \n",
      "4  Alabama () is a state in the Southeastern regi...  \n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Convert a specific split to DataFrame\n",
    "train_df = data['train'].to_pandas()\n",
    "\n",
    "# Now you can view the DataFrame\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7aff4c-4a69-42c1-b5e9-d7eda1c7ab9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6458670"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b707768f-6bf7-42b1-93bd-cc6fec995d74",
   "metadata": {},
   "source": [
    "## Sampling out 99999 Rows initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad67df69-7fbd-46d0-8bcd-a98d93c5d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_df is your original DataFrame\n",
    "df = train_df.sample(n=99999, random_state=42)  # random_state for reproducibility\n",
    "\n",
    "# Now df contains a random sample of 2000 rows from train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ad31872-975c-4fea-9b58-92e8862394e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = df['text'].apply(str)  # Ensure all data is string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b532230-89ac-4b13-a484-e13bad68107b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5061375    Manmohan Tiwari (born 20 January 1984) is an I...\n",
       "558397     Everything Is Wrong is the third studio album ...\n",
       "4552231    Avalanche Canada is a non-government, non-prof...\n",
       "297554     The fukiya (吹き矢) is the Japanese blowgun, as w...\n",
       "426754     Vasanta or Vasantha may refer to:\\n\\n Vasanta ...\n",
       "                                 ...                        \n",
       "6209050    John J. Cronin is a Massachusetts state senato...\n",
       "4824072    Radiate is a mobile app that connects people g...\n",
       "4526894    Eddie Castrodad is an American former film, te...\n",
       "4309994    Qaleh-ye Chum (, also Romanized as Qal‘eh-ye C...\n",
       "3288019    Mahogany Bluff () is a rocky bluff  southwest ...\n",
       "Name: text, Length: 99999, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d71fc-b154-46f3-942a-ee341a8b9df6",
   "metadata": {},
   "source": [
    "### Having sampled out about 99k rows we will move to pre-process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99811eaa-8e29-46a8-8721-5f428bcc6f17",
   "metadata": {},
   "source": [
    "### Pre-processing the text, cleaning and preparing it for \n",
    "\n",
    "1) Lowercase all text.\n",
    "2) Remove Punctuations.\n",
    "3) Leaverged Stopwords API from NLTK to remove unnecessary stopwords.\n",
    "4) Leaveraged word tokenizer API from NLTK to tokenise the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93f0aa0f-cb61-42a7-b10e-69c24d72aff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/km.s/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/km.s/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c0af4-cede-40e9-97f6-07ea1ada17fc",
   "metadata": {},
   "source": [
    "## Applying Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edef25a4-bdac-4e75-8e29-1b2a90459649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing steps\n",
    "preprocessed_text = text_data.apply(lowercase).apply(remove_punctuation).apply(remove_stopwords)\n",
    "tokens = preprocessed_text.apply(tokenize)\n",
    "\n",
    "# Flattening the list of tokens for building vocabulary\n",
    "flat_list_tokens = [item for sublist in tokens.tolist() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84f011ba-acbd-44b5-8cc7-d5f7d459ec49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30582358"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_list_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af7f5d07-33d1-4a3f-9227-8204e7cbee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [text.split() for text in preprocessed_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c5a2c24-fe08-4a0d-b2c2-1831e809aeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99999"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc5d5a54-090d-449e-a12b-d64fc1fc0796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec4d42e1-1b2e-46d9-978f-55e94f079046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 99999 lists inside the list.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a count\n",
    "count_of_lists = 0\n",
    "\n",
    "# Loop through each item in text_list\n",
    "for item in text_list:\n",
    "    if isinstance(item, list):  # Check if the item is a list\n",
    "        count_of_lists += 1\n",
    "\n",
    "print(f\"There are {count_of_lists} lists inside the list.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d7b2a-eb79-4d66-9aa0-e164491bd3ed",
   "metadata": {},
   "source": [
    "### Approach 2: Continous Bag of Words (CBOW)\n",
    "\n",
    "* Continuous skip-gram model: predicts words within a certain range before and after the current word in the same sentence. A worked example of this is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c698a68-7887-4b8f-be9e-d9156a03a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing steps\n",
    "preprocessed_text = text_data.apply(lowercase).apply(remove_punctuation).apply(remove_stopwords)\n",
    "tokens = preprocessed_text.apply(tokenize)\n",
    "\n",
    "# Flattening the list of tokens for building vocabulary\n",
    "flat_list_tokens = [item for sublist in tokens.tolist() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa498790-19e6-4695-963a-36ddeca59e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each row of text into a list of words\n",
    "text_list = [text.split() for text in preprocessed_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8058457d-b5c4-4996-a514-298b2abbdfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1040482\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(tokens):\n",
    "    vocab = {word: idx for idx, word in enumerate(set(tokens))}\n",
    "    return vocab\n",
    "\n",
    "def encode_tokens(tokens, vocab):\n",
    "    encoded = [vocab[token] for token in tokens]\n",
    "    return encoded\n",
    "\n",
    "vocab = build_vocab(flat_list_tokens)\n",
    "encoded_tokens = [encode_tokens(token_list, vocab) for token_list in tokens]\n",
    "\n",
    "# Example output\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "375ef9c2-8d9a-4621-86a5-c1b5c6cb3ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999\n"
     ]
    }
   ],
   "source": [
    "number_of_lists = sum(isinstance(item, list) for item in encoded_tokens)\n",
    "print(number_of_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf58d7d-3451-4a11-af35-afa77a095ba9",
   "metadata": {},
   "source": [
    "### Building Word2Vec Model\n",
    "With our data preprocessed, now we define and train our Word2Vec model. I have used the Gensim library, which is straightforward for training Word2Vec models.\n",
    "\n",
    "### Word2Vec Parameters\n",
    "* **sentences** : List of Lists inside which preprocessed text sits, which is used by GENSIM to train word2vec based on the context and sematic relationships captured\n",
    "\n",
    "* **vector_size**: It is the size of dimension's (2D, 3D, in our case 100D) which is being initialized to help the model catch complex semntic relationships\n",
    "\n",
    "* **window**: number of words to be observed to catch context. The window size determines the span of words on either side of a target_word that can be considered a context word\n",
    "\n",
    "* **sg**: Setting the Approach (1-Skip Gram approach, else if the status is set to \"0\", then it is CBOW approach)\n",
    "\n",
    "* **min_count**: \n",
    "\n",
    "* **epochs**: Number of Loops over the data for \n",
    "\n",
    "Also we will be building models with both CBOW and Skip-gram. And we will compare the performance of the two models on this dataset alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fdec25e-7941-4be7-b50c-4a84dddb7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Assuming 'sentences' is your preprocessed and tokenized text data\n",
    "cbow_model = Word2Vec(sentences = text_list, vector_size=100, window=5, sg=0, min_count=1)\n",
    "# cbow_model.train(encoded_tokens, total_examples=len(encoded_tokens), epochs=10)\n",
    "\n",
    "# Now you can compare Skip-gram and CBOW based on:\n",
    "# - Quality of the embeddings via similarity measures or analogy tasks\n",
    "# - Performance on downstream tasks (if any)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b6b36-3e29-4acb-af37-23587676fdeb",
   "metadata": {},
   "source": [
    "* Training the CBOW model was significantly quicker than the skip gram approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6abcd9bf-b762-48fb-b88a-b76804f1aa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram model similarity between sun and cloth: 0.22371233999729156\n"
     ]
    }
   ],
   "source": [
    "# Words to compare\n",
    "word1 = 'sun'\n",
    "word2 = 'cloth'\n",
    "\n",
    "# Check if both words are in the vocabulary of the model (assuming CBOW model here)\n",
    "if word1 in cbow_model.wv.key_to_index and word2 in cbow_model.wv.key_to_index:\n",
    "    similarity_skip_gram = cbow_model.wv.similarity(word1, word2)\n",
    "    print(f'Skip-gram model similarity between {word1} and {word2}: {similarity_skip_gram}')\n",
    "else:\n",
    "    print(f'One or both words not in Skip-gram model vocabulary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef1099f6-9036-4dce-8a07-d99c7e20745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 words closest to 'example':\n",
      "instance: 0.8416684865951538\n",
      "examples: 0.7914485335350037\n",
      "likewise: 0.7651951313018799\n",
      "particular: 0.7328760027885437\n",
      "contrast: 0.7312443852424622\n",
      "clearly: 0.7232483625411987\n",
      "similarly: 0.7210996150970459\n",
      "necessarily: 0.7077968716621399\n",
      "manner: 0.6973150372505188\n",
      "rather: 0.695795476436615\n"
     ]
    }
   ],
   "source": [
    "# Specify your input word\n",
    "input_word = 'example'  # Replace 'example' with your actual input word\n",
    "\n",
    "# Assuming 'model' is your Skip-gram trained model\n",
    "# Find the 10 words closest to the input word\n",
    "closest_words = cbow_model.wv.most_similar(input_word, topn=10)\n",
    "\n",
    "print(f\"10 words closest to '{input_word}':\")\n",
    "for word, similarity in closest_words:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "318da064-7e89-46ac-9dd5-ec3c70fd6a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words in Skip-gram model's vocabulary:\n",
      "['also', 'first', 'new', 'references', 'one', 'people', 'american', 'two', 'united', 'university']\n"
     ]
    }
   ],
   "source": [
    "# List first 10 words in Skip-gram model's vocabulary\n",
    "skip_gram_vocab = list(cbow_model.wv.key_to_index.keys())\n",
    "print(\"First 10 words in Skip-gram model's vocabulary:\")\n",
    "print(skip_gram_vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "486ad7e3-1ce8-412d-99be-6d5756abdccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 words least similar to 'sun':\n",
      "zistersdorf: -0.4427976906299591\n",
      "arithmomaniac: -0.44182586669921875\n",
      "mattani: -0.42715543508529663\n",
      "schünemanns: -0.4065355360507965\n",
      "hostagetakers: -0.40505439043045044\n",
      "atiglio: -0.4049690365791321\n",
      "ollerías: -0.4011055529117584\n",
      "acalyphus: -0.4006273150444031\n",
      "ghid: -0.39958053827285767\n",
      "savene: -0.3926926553249359\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Specify your input word\n",
    "input_word = 'sun'  # Replace 'example' with your actual input word\n",
    "\n",
    "# Assuming 'model' is your Skip-gram trained model\n",
    "# Retrieve all words from the model's vocabulary\n",
    "all_words = list(cbow_model.wv.key_to_index.keys())\n",
    "\n",
    "# Calculate similarity of input word with all other words in the vocabulary\n",
    "similarities = [(word, cbow_model.wv.similarity(input_word, word)) for word in all_words]\n",
    "\n",
    "# Sort the words by similarity in ascending order (least similar first)\n",
    "least_similar_words = sorted(similarities, key=lambda x: x[1])\n",
    "\n",
    "# Display the 10 least similar words to the input word\n",
    "print(f\"10 words least similar to '{input_word}':\")\n",
    "for word, similarity in least_similar_words[:10]:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f43c897-8d22-4a5e-ac58-027d929993ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between king and queen: 0.46683260798454285\n"
     ]
    }
   ],
   "source": [
    "similarity_king_queen = cbow_model.wv.similarity('man', 'women')\n",
    "print(f'Similarity between king and queen: {similarity_king_queen}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cfe9db2c-2593-418f-b5ac-73f1c1a9cab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65765923"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.similarity('laptop'.lower(), 'pc'.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d8ccaa1-1ed4-4f3a-996b-5390947d24e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7001268863677979)]\n"
     ]
    }
   ],
   "source": [
    "print(cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6acdd047-de7b-48da-8ca8-097ef8bae41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man is to king as woman is to queen\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define the analogy components\n",
    "positive = ['king', 'woman']\n",
    "negative = ['man']\n",
    "\n",
    "# Find the words that complete the analogy\n",
    "result = cbow_model.wv.most_similar(positive=positive, negative=negative, topn=1)\n",
    "\n",
    "# Print the result\n",
    "print(f\"man is to king as woman is to {result[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4499d3-c2fe-4ee4-9028-c784f484a835",
   "metadata": {},
   "source": [
    "This test proves that the model has been able to learn and performs fine on Analogy tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e603067e-1271-4ca0-b625-152066432a53",
   "metadata": {},
   "source": [
    "## Odd one out Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "080e759a-0e9b-4834-8037-475ca4a44109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model vectors and metadata for projector\n",
    "# saved_vectors = cbow_model.wv.save_word2vec_format('vecs.tsv', 'meta.tsv', binary=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
