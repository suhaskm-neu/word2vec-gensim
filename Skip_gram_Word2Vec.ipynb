{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc660e-d4b0-4490-8810-29293e19315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install gensim\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib\n",
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741aa3b-1c98-4366-8e83-cbcbf91f7104",
   "metadata": {},
   "source": [
    "### Loading the wikipedia English text Data available from Hugging Face\n",
    "\n",
    "https://huggingface.co/datasets/wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677a8cf1-054a-45d2-91cb-311fa3e10137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"wikipedia\", \"20220301.en\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f10b23-1dfe-427d-9aa3-74453a8c832f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18d5e89-1592-45c1-a4d3-97afa3c01c52",
   "metadata": {},
   "source": [
    "Converting DatasetDict type to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f93abbef-c475-450e-94d1-8edb09cfdde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                      url      title  \\\n",
      "0   12  https://en.wikipedia.org/wiki/Anarchism  Anarchism   \n",
      "1   25     https://en.wikipedia.org/wiki/Autism     Autism   \n",
      "2   39     https://en.wikipedia.org/wiki/Albedo     Albedo   \n",
      "3  290          https://en.wikipedia.org/wiki/A          A   \n",
      "4  303    https://en.wikipedia.org/wiki/Alabama    Alabama   \n",
      "\n",
      "                                                text  \n",
      "0  Anarchism is a political philosophy and moveme...  \n",
      "1  Autism is a neurodevelopmental disorder charac...  \n",
      "2  Albedo (; ) is the measure of the diffuse refl...  \n",
      "3  A, or a, is the first letter and the first vow...  \n",
      "4  Alabama () is a state in the Southeastern regi...  \n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Convert a specific split to DataFrame\n",
    "train_df = data['train'].to_pandas()\n",
    "\n",
    "# Now you can view the DataFrame\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "659f2175-3f71-476f-8c71-5d4bbe02af24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6458670"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3123d6b-fd3f-43f3-acde-4b6b66558b2f",
   "metadata": {},
   "source": [
    "## Sampling out 99999 Rows initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f28b905-a749-4465-8f6d-3bb68d937804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_df is your original DataFrame\n",
    "df = train_df.sample(n=99999, random_state=42)  # random_state for reproducibility\n",
    "\n",
    "# Now df contains a random sample of 99999 rows from train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82034881-a1fb-4d42-aa2e-21d7c8c53d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = df['text'].apply(str)  # Ensure all data is string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0983a8ff-95d1-4864-9a92-44d8b0f4f3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5061375    Manmohan Tiwari (born 20 January 1984) is an I...\n",
       "558397     Everything Is Wrong is the third studio album ...\n",
       "4552231    Avalanche Canada is a non-government, non-prof...\n",
       "297554     The fukiya (吹き矢) is the Japanese blowgun, as w...\n",
       "426754     Vasanta or Vasantha may refer to:\\n\\n Vasanta ...\n",
       "                                 ...                        \n",
       "6209050    John J. Cronin is a Massachusetts state senato...\n",
       "4824072    Radiate is a mobile app that connects people g...\n",
       "4526894    Eddie Castrodad is an American former film, te...\n",
       "4309994    Qaleh-ye Chum (, also Romanized as Qal‘eh-ye C...\n",
       "3288019    Mahogany Bluff () is a rocky bluff  southwest ...\n",
       "Name: text, Length: 99999, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831abafc-93b3-458d-b4a6-1d781bc9524d",
   "metadata": {},
   "source": [
    "### Having sampled out about 99k rows we will move to pre-process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43551e7a-6750-4c11-8b4b-2c8fd0a57e92",
   "metadata": {},
   "source": [
    "### Pre-processing the text, cleaning and preparing it for \n",
    "\n",
    "1) Lowercase all text.\n",
    "2) Remove Punctuations.\n",
    "3) Leaverged Stopwords API from NLTK to remove unnecessary stopwords.\n",
    "4) Leaveraged word tokenizer API from NLTK to tokenise the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94887732-97ce-4875-b644-7ce81df9c8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/km.s/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/km.s/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1d3590-07b3-4e7c-8e63-b3cc55bb7f97",
   "metadata": {},
   "source": [
    "## Applying Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9adf26c6-9ba3-480f-96a4-05e775044077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing steps\n",
    "preprocessed_text = text_data.apply(lowercase).apply(remove_punctuation).apply(remove_stopwords)\n",
    "tokens = preprocessed_text.apply(tokenize)\n",
    "\n",
    "# Flattening the list of tokens for building vocabulary\n",
    "flat_list_tokens = [item for sublist in tokens.tolist() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a25be7f-1c86-4d9e-8dce-7b8905130c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30582358"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_list_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c20ee1b1-773c-4ba6-9f7d-419bb7002b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(flat_list_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c524f0cd-6953-4867-946b-8bdcc12a28a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5061375    manmohan tiwari born 20 january 1984 indian te...\n",
       "558397     everything wrong third studio album american e...\n",
       "4552231    avalanche canada nongovernment nonprofit organ...\n",
       "297554     fukiya 吹き矢 japanese blowgun well term associat...\n",
       "426754     vasanta vasantha may refer vasanta ritu basant...\n",
       "                                 ...                        \n",
       "6209050    john j cronin massachusetts state senator repr...\n",
       "4824072    radiate mobile app connects people going event...\n",
       "4526894    eddie castrodad american former film televisio...\n",
       "4309994    qalehye chum also romanized qalehye chūm villa...\n",
       "3288019    mahogany bluff rocky bluff southwest cape gord...\n",
       "Name: text, Length: 99999, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cac67fbe-b558-443b-b752-105e8d0874b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each row of text into a list of words\n",
    "text_list = [text.split() for text in preprocessed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc35369-a799-4c1b-8de6-3fb33fb7010e",
   "metadata": {},
   "source": [
    "# Text ready for GENSIM\n",
    "* Now text_list is in the correct format for Word2Vec\n",
    "* It is a list of lists, where each sublist is a sequence of words (tokens) from a single document, which can be fed into the GENSIM Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a9b00-dd25-42c8-8ad0-fcfaac229789",
   "metadata": {},
   "source": [
    "NOTE: Processing main dataset - the list of Tokens have now taken up more than 3 hours... Moving to a simpler Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f370e75a-056e-42d5-86df-753041d11b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1040482\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(tokens):\n",
    "    vocab = {word: idx for idx, word in enumerate(set(tokens))}\n",
    "    return vocab\n",
    "\n",
    "def encode_tokens(tokens, vocab):\n",
    "    encoded = [vocab[token] for token in tokens]\n",
    "    return encoded\n",
    "\n",
    "vocab = build_vocab(flat_list_tokens)\n",
    "encoded_tokens = [encode_tokens(token_list, vocab) for token_list in tokens]\n",
    "\n",
    "# Example output\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "115e078f-1dfe-4f36-9286-631ad671b10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999\n"
     ]
    }
   ],
   "source": [
    "number_of_lists = sum(isinstance(item, list) for item in encoded_tokens)\n",
    "print(number_of_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311dc06e-6d95-4c58-9307-c62e3c6dd608",
   "metadata": {},
   "source": [
    "### Building Word2Vec Model\n",
    "With our data preprocessed, now we define and train our Word2Vec model. I have used the Gensim library, which is straightforward for training Word2Vec models.\n",
    "\n",
    "### Word2Vec Parameters\n",
    "* **sentences** : List of Lists inside which preprocessed text sits, which is used by GENSIM to train word2vec based on the context and sematic relationships captured\n",
    "\n",
    "* **vector_size**: It is the size of dimension's (2D, 3D, in our case 100D) which is being initialized to help the model catch complex semntic relationships\n",
    "\n",
    "* **window**: number of words to be observed to catch context. The window size determines the span of words on either side of a target_word that can be considered a context word\n",
    "\n",
    "* **sg**: Setting the Approach (1-Skip Gram approach, else if the status is set to \"0\", then it is CBOW approach)\n",
    "\n",
    "* **min_count**: \n",
    "\n",
    "* **epochs**: Number of Loops over the data for \n",
    "\n",
    "Also we will be building models with both CBOW and Skip-gram. And we will compare the performance of the two models on this dataset alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5de0de-9b2f-40d9-b9f8-3dd0af59e697",
   "metadata": {},
   "source": [
    "### Approach 1: Skip-Gram Approach\n",
    "\n",
    "* Continuous bag-of-words model: predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad09c19d-ddb4-4df2-a67f-fb1fe9c953e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define Word2Vec model - Using Skip-gram (sg=1)\n",
    "model = Word2Vec(sentences=text_list, vector_size=100, window=5, sg=1, min_count=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b37b42ab-346d-451a-97b6-0ded2380d1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man is to king as woman is to throne\n"
     ]
    }
   ],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# Assuming your Gensim model is loaded in the variable `model`\n",
    "\n",
    "# Define the analogy components\n",
    "positive = ['king', 'woman']\n",
    "negative = ['man']\n",
    "\n",
    "# Find the words that complete the analogy\n",
    "result = model.wv.most_similar(positive=positive, negative=negative, topn=1)\n",
    "\n",
    "# Print the result\n",
    "print(f\"man is to king as woman is to {result[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc3e2d-9cca-4ca3-a5c9-c770268a5eb5",
   "metadata": {},
   "source": [
    "## The Skip-gram approach has failed the Analogy test. Whereas thr CBOW approach performed fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb09b7-c027-4f62-ac6c-02e46f78154b",
   "metadata": {},
   "source": [
    "## Next we will be comparing both the approaches. \n",
    "\n",
    "A brief summary might include observations on which model captures semantic relationships better, their training time comparison, and any differences in performance on specific tasks (e.g., analogy solving).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb061dd7-7107-42d0-8247-f718d260cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram model similarity between sun and cloth: 0.336364209651947\n"
     ]
    }
   ],
   "source": [
    "# Words to compare\n",
    "word1 = 'sun'\n",
    "word2 = 'cloth'\n",
    "\n",
    "# Check if both words are in the vocabulary of the model (assuming Skip-gram model here)\n",
    "if word1 in model.wv.key_to_index and word2 in model.wv.key_to_index:\n",
    "    similarity_skip_gram = model.wv.similarity(word1, word2)\n",
    "    print(f'Skip-gram model similarity between {word1} and {word2}: {similarity_skip_gram}')\n",
    "else:\n",
    "    print(f'One or both words not in Skip-gram model vocabulary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d74e23f-1101-436a-8e26-a1f154caf8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 words closest to 'example':\n",
      "instance: 0.9114886522293091\n",
      "examples: 0.8914328217506409\n",
      "particular: 0.8338468074798584\n",
      "contrast: 0.8154786229133606\n",
      "consider: 0.8134335875511169\n",
      "likewise: 0.8124775886535645\n",
      "similarly: 0.8118988871574402\n",
      "therefore: 0.8118278384208679\n",
      "necessarily: 0.8104322552680969\n",
      "possible: 0.8036367297172546\n"
     ]
    }
   ],
   "source": [
    "# Specify your input word\n",
    "input_word = 'example'\n",
    "\n",
    "# Assuming 'model' is your Skip-gram trained model\n",
    "# Find the 10 words closest to the input word\n",
    "closest_words = model.wv.most_similar(input_word, topn=10)\n",
    "\n",
    "print(f\"10 words closest to '{input_word}':\")\n",
    "for word, similarity in closest_words:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "652e8a36-6dea-4429-a6b1-4d2e4f589810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 words least similar to 'sun':\n",
      "id977: -0.3495708703994751\n",
      "kargo: -0.3426513969898224\n",
      "sw20: -0.3407485783100128\n",
      "421778: -0.32830116152763367\n",
      "manogi: -0.3118351399898529\n",
      "460836: -0.30719104409217834\n",
      "id919: -0.2978152334690094\n",
      "04490: -0.29093706607818604\n",
      "id971: -0.28943178057670593\n",
      "846th: -0.2891865372657776\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Specify your input word\n",
    "input_word = 'sun'  # Replace 'example' with your actual input word\n",
    "\n",
    "# Assuming 'model' is your Skip-gram trained model\n",
    "# Retrieve all words from the model's vocabulary\n",
    "all_words = list(model.wv.key_to_index.keys())\n",
    "\n",
    "# Calculate similarity of input word with all other words in the vocabulary\n",
    "similarities = [(word, model.wv.similarity(input_word, word)) for word in all_words]\n",
    "\n",
    "# Sort the words by similarity in ascending order (least similar first)\n",
    "least_similar_words = sorted(similarities, key=lambda x: x[1])\n",
    "\n",
    "# Display the 10 least similar words to the input word\n",
    "print(f\"10 words least similar to '{input_word}':\")\n",
    "for word, similarity in least_similar_words[:10]:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77bc6c2b-835c-4094-b238-534267d52cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words in Skip-gram model's vocabulary:\n",
      "['also', 'first', 'new', 'references', 'one', 'people', 'american', 'two', 'united', 'university']\n"
     ]
    }
   ],
   "source": [
    "# List first 10 words in Skip-gram model's vocabulary\n",
    "skip_gram_vocab = list(model.wv.key_to_index.keys())\n",
    "print(\"First 10 words in Skip-gram model's vocabulary:\")\n",
    "print(skip_gram_vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef69df-a108-4c3d-af96-8c4f47cabf48",
   "metadata": {},
   "source": [
    "## Looks like we found the Issue\n",
    "\n",
    "suggests that the words in your models' vocabularies are represented by numerical IDs rather than by the actual words themselves. This typically happens when the input data to the Word2Vec model is preprocessed into numerical form (e.g., indexed or encoded) rather than being left as raw text tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51171d7-6d30-427e-85fc-abb19bfd9d3a",
   "metadata": {},
   "source": [
    "Debugging: If retraining is not immediately feasible or if you're curious about the current state of your models, you might want to investigate further by checking the preprocessing steps to see how these numeric IDs were generated from text. This could involve reviewing the tokenization, encoding, or any custom preprocessing logic that we have applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be769412-c082-46f0-967f-5274caab9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct format: List of sentences, where each sentence is a list of word tokens\n",
    "correct_format_example = [['hello', 'world'], ['word2vec', 'model']]\n",
    "\n",
    "# Then, train your models again with this format\n",
    "skip_gram_model = Word2Vec(sentences=correct_format_example, vector_size=100, window=5, sg=1, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc55df1-db5f-45d4-8e25-8894cda26643",
   "metadata": {},
   "source": [
    "# Step 3: Evaluate Word Embeddings\n",
    "3.1 Compute Similarity Between Word Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d097873c-14e7-41e0-9d21-6a08c63ef7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between king and queen: 0.5139315724372864\n"
     ]
    }
   ],
   "source": [
    "similarity_king_queen = model.wv.similarity('man', 'women')\n",
    "print(f'Similarity between king and queen: {similarity_king_queen}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3cdb0dd-43b6-4a40-b298-e1ae73950d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('studio' in model.wv.key_to_index)  # For Gensim version 4.x\n",
    "# or for older versions: 'canada' in model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60851f09-15e0-4984-8626-7bc4d03489e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52500683"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('laptop'.lower(), 'pc'.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f4c4d43-0a3b-41ab-beb2-d1c9a539b9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('throne', 0.7363272309303284)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0c8af-dc4e-4b4c-8e5d-c7711a756bf6",
   "metadata": {},
   "source": [
    "## 3.3 Visualization\n",
    "\n",
    "For visualization, you can use the Embedding Projector by exporting the model's embeddings and metadata. Here's a quick guide to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89c4573a-398b-4454-a041-97a4778bc0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model vectors and metadata for projector\n",
    "# saved_vectors = model.wv.save_word2vec_format('vecs.tsv', 'meta.tsv', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cdd587-8941-4227-ba87-45dd1f4de68c",
   "metadata": {},
   "source": [
    "Now, Upload vecs.tsv and meta.tsv to the TensorFlow Embedding Projector for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d5ee6-e6d7-4ccf-98e5-44bc44b29183",
   "metadata": {},
   "source": [
    "* Save your model vectors and metadata correctly for the TensorFlow Embedding Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce56686-db52-45a9-9865-b7d2839287ef",
   "metadata": {},
   "source": [
    "## This provide qualitative evidence of your model's performance and insights into the data it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ffedf8-3445-42c5-ac5d-b434ded423e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assume you want to create 10 clusters\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(vectors)\n",
    "\n",
    "# Check the cluster assignments for each word\n",
    "word_clusters = kmeans.labels_\n",
    "for word, cluster in zip(words, word_clusters):\n",
    "    print(f\"{word}: Cluster {cluster}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05751819-4fc2-444a-a0c4-947f36d389df",
   "metadata": {},
   "source": [
    "alternative ways to visualize and evaluate your Word2Vec embeddings using Python libraries such as matplotlib, seaborn, and especially plotly for interactive visualizations. Additionally, you can use dimensionality reduction techniques like PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to reduce the embeddings to two or three dimensions, which are easier to visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8184f70-cb4e-4007-ba1d-579342bc2fed",
   "metadata": {},
   "source": [
    "### Visualization with PCA or t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae63a5-0601-484b-8f38-b7a0c6076d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the word vectors from the model\n",
    "words = list(model.wv.key_to_index.keys())\n",
    "vectors = [model.wv[word] for word in words]\n",
    "\n",
    "# Use t-SNE to reduce dimensionality\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "vectors_2d = tsne.fit_transform(vectors)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], edgecolors='k', c='r')\n",
    "for word, (x, y) in zip(words, vectors_2d):\n",
    "    plt.text(x, y, word)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "235c9265-8648-40ea-a208-4b6788315ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('throne', 0.7547318935394287)]\n"
     ]
    }
   ],
   "source": [
    "result = model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ceeccba-b87d-47bc-b90a-05fb22e76f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have a Word2Vec model named 'model'\n",
    "vectors = np.array([model.wv[word] for word in model.wv.key_to_index.keys()])\n",
    "\n",
    "# Save the vectors to a .npy file\n",
    "np.save('skip_gram_word_embeddings.npy', vectors)\n",
    "\n",
    "# Optionally, you might also want to save the corresponding words (metadata) to a separate file\n",
    "words = list(model.wv.key_to_index.keys())\n",
    "with open('metadata.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in words:\n",
    "        f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606fc3ce-6133-48a3-9d13-64c1d87351d4",
   "metadata": {},
   "source": [
    "The .npy file format, which is a binary file format for storing NumPy arrays, offers a convenient way to persist and later access your Word2Vec embeddings in Python. Here are several scenarios where the .npy file containing your word embeddings might be useful:\n",
    "\n",
    "1. Transfer Learning and Feature Extraction\n",
    "Machine Learning Models: Use the word embeddings as features in machine learning models for various NLP tasks such as sentiment analysis, text classification, or clustering. Embeddings can provide rich, pre-trained representations of words that capture semantic relationships, improving model performance.\n",
    "\n",
    "Deep Learning Architectures: Embeddings can be used as the initial layer in deep learning models for tasks like sequence modeling, language translation, or text generation. Using pre-trained embeddings can speed up training and improve model performance, especially when you have limited labeled data for training.\n",
    "\n",
    "2. Data Analysis and Visualization\n",
    "Similarity and Clustering Analyses: Load the embeddings to perform cosine similarity calculations between words, cluster words based on their embeddings, or identify outlier words. These analyses can reveal semantic structures in your dataset.\n",
    "\n",
    "Visualization: Beyond TensorFlow's Embedding Projector, you can use tools like t-SNE or PCA for dimensionality reduction on the embeddings and then visualize them with libraries such as matplotlib, seaborn, or plotly. This can help you understand the embeddings' quality and the semantic relationships they've captured.\n",
    "\n",
    "3. Natural Language Processing Applications\n",
    "Information Retrieval: Enhance search algorithms or recommender systems by using word embeddings to find semantically related documents or items based on textual descriptions.\n",
    "\n",
    "Text Generation and Auto-completion: Implement models that generate text or complete sentences where embeddings serve as a foundational layer, capturing linguistic nuances.\n",
    "\n",
    "4. Combining with Other Models\n",
    "Ensemble Models: Combine the embeddings with other data sources and models. For example, use embeddings alongside image features in a multimodal model that understands both text and visual input.\n",
    "5. Educational and Experimental Purposes\n",
    "Learning and Experimentation: Load the embeddings in different environments to experiment with new NLP libraries or to teach concepts related to word embeddings and their applications.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "* Load the embeddings\n",
    "embeddings = np.load('word_embeddings.npy')\n",
    "\n",
    "* If you've also saved the words in a separate file, you can load them to map embeddings back to words\n",
    "with open('metadata.txt', 'r', encoding='utf-8') as f:\n",
    "    words = [line.strip() for line in f]\n",
    "\n",
    "* Example: Access the embedding for the first word\n",
    "first_word_embedding = embeddings[0]\n",
    "print(f\"Embedding for the first word: {first_word_embedding}\")\n",
    "\n",
    "\n",
    "\n",
    "**Storing your embeddings in an .npy file thus provides a versatile, efficient way to reuse pre-trained word representations across a wide range of applications and analyses, enhancing the overall NLP workflow.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a980e2a-57c4-4882-8130-26bc2371046d",
   "metadata": {},
   "source": [
    "**Performing Analogy Reasoning**\n",
    "\n",
    "Analogy reasoning involves finding a word that relates to another word in the same way that a third word relates to a fourth. For example, \"man\" is to \"king\" as \"woman\" is to \"queen\". To perform this task with your embeddings, you need the embeddings loaded (as you have) and a way to compute similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "093bae68-a0bb-4646-be2d-666f6b8f4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming you have a function to get the embedding for a word\n",
    "def get_embedding(word, word_index, embeddings):\n",
    "    idx = word_index[word]\n",
    "    return embeddings[idx]\n",
    "\n",
    "# Function to find the closest word for the vector created by \"word_a - word_b + word_c\"\n",
    "def find_closest_word(word_a, word_b, word_c, word_index, embeddings):\n",
    "    embedding_a = get_embedding(word_a, word_index, embeddings)\n",
    "    embedding_b = get_embedding(word_b, word_index, embeddings)\n",
    "    embedding_c = get_embedding(word_c, word_index, embeddings)\n",
    "    analogy_vector = embedding_a - embedding_b + embedding_c\n",
    "\n",
    "    # Compute similarity with all words\n",
    "    similarities = cosine_similarity([analogy_vector], embeddings)[0]\n",
    "    \n",
    "    # Exclude the original words from consideration\n",
    "    for word in [word_a, word_b, word_c]:\n",
    "        idx = word_index[word]\n",
    "        similarities[idx] = -1\n",
    "        \n",
    "    closest_word_idx = similarities.argmax()\n",
    "    # Assuming you have a list or array `index_word` that maps indices back to words\n",
    "    return index_word[closest_word_idx]\n",
    "\n",
    "# Example usage:\n",
    "# word_index = {word: idx for idx, word in enumerate(words)}  # Assuming 'words' is your list of words\n",
    "# index_word = {idx: word for word, idx in word_index.items()}\n",
    "# print(find_closest_word('king', 'man', 'woman', word_index, embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763d6aa-d354-4f57-bf2f-45ef00802905",
   "metadata": {},
   "source": [
    "This method is effective for exploring semantic relationships captured by your embeddings. To get this code running:\n",
    "\n",
    "Ensure you have precomputed word embeddings (embeddings) and a mapping of words to their indices in the embedding matrix (word_index and its inverse index_word).\n",
    "Make sure scikit-learn is installed for cosine_similarity.\n",
    "The example usage provided in the comment is a good starting point. You would just need to replace the placeholders (word_index, index_word, embeddings) with your actual data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d1594-7352-4e2b-a6d2-31df7c001231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
